{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "query = torch.FloatTensor([[1,2,3],[4,5,6],[7,8,9],[3,2,3],[3,2,1],[1,2,3]])\n",
    "key = torch.FloatTensor([[2.2,4,3],[6,9,1],[8,2,9],[1,4,5],[2,4,5],[3,2,9]])\n",
    "value = torch.FloatTensor([[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[4,8,9]])\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query,key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        # masked_fill(mask,value)\n",
    "        # fills elements of self tensor with value where mask is one\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores,dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "        \n",
    "    # value에 확률값 곱하고 더한 벡터, 확률값\n",
    "    return torch.matmul(p_attn,value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1582, 1.3692, 1.4219],\n",
       "         [1.0000, 1.0001, 1.0001],\n",
       "         [1.0000, 1.0000, 1.0000],\n",
       "         [1.0005, 1.0012, 1.0014],\n",
       "         [1.0003, 1.0006, 1.0007],\n",
       "         [1.1582, 1.3692, 1.4219]]),\n",
       " tensor([[1.0261e-05, 9.2672e-04, 9.4587e-01, 1.6396e-04, 2.9206e-04, 5.2739e-02],\n",
       "         [4.6087e-13, 5.4256e-06, 9.9998e-01, 2.9438e-11, 2.9640e-10, 9.6648e-06],\n",
       "         [1.9581e-20, 3.0047e-08, 1.0000e+00, 4.9996e-18, 2.8452e-16, 1.6753e-09],\n",
       "         [1.3386e-08, 9.7284e-05, 9.9973e-01, 5.3508e-08, 3.0244e-07, 1.7329e-04],\n",
       "         [6.8323e-06, 4.9994e-01, 4.9994e-01, 2.7126e-06, 1.5332e-05, 8.6660e-05],\n",
       "         [1.0261e-05, 9.2672e-04, 9.4587e-01, 1.6396e-04, 2.9206e-04, 5.2739e-02]]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # linear projections in batch from d_model\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "                            for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention(query, key, value, mask = mask,\n",
    "                                dropout = self.dropout)\n",
    " \n",
    "        # concat using a view and apply a final linear\n",
    "        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25, 25, 14, 14],\n",
       "        [25, 25, 14, 14],\n",
       "        [25, 25, 14, 14],\n",
       "        [25, 25, 14, 14]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() missing 1 required positional arguments: \"dim1\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-3a5499522408>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: transpose() missing 1 required positional arguments: \"dim1\""
     ]
    }
   ],
   "source": [
    "query = torch.tensor([[1,2,3],[1,2,3]])\n",
    "query = torch.tensor([[1,2,3]])\n",
    "\n",
    "query.transpose(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3]]) \n",
      " tensor([[2, 2],\n",
      "        [4, 4],\n",
      "        [5, 5]])\n"
     ]
    }
   ],
   "source": [
    "query = np.array([[1,2,3],[1,2,3]])\n",
    "query = torch.tensor([[1,2,3],[1,2,3]])\n",
    "\n",
    "print(query,\"\\n\", key.transpose(-2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = \\\n",
    "        [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        for l, x in zip(self.linears, (query, key, value))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
